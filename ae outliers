
@torch.no_grad()
def encode_latent(model, X_scaled: np.ndarray, batch_size: int = 1024):
    model.eval()
    Zs = []
    for i in range(0, len(X_scaled), batch_size):
        xb = torch.from_numpy(X_scaled[i:i+batch_size]).float()
        zb = model.encoder(xb)
        Zs.append(zb.cpu().numpy())
    return np.concatenate(Zs, axis=0)

def fit_gaussian(Z, eps=1e-6):
    mu = Z.mean(axis=0)
    C  = np.cov(Z, rowvar=False) + eps*np.eye(Z.shape[1])
    Cinv = np.linalg.inv(C)
    return mu, Cinv

def mahalanobis(Z, mu, Cinv):
    D = Z - mu
    return np.sqrt(np.einsum("ij,jk,ik->i", D, Cinv, D))

import numpy as np
import torch

@torch.no_grad()
def recon_error_mse(model, X_scaled: np.ndarray, scaler=None, batch_size: int = 1024,
                    return_recons: bool = False):
    model.eval()
    outs = []
    for i in range(0, len(X_scaled), batch_size):
        xb = torch.from_numpy(X_scaled[i:i+batch_size]).float()
        xh = model(xb)
        outs.append(xh.cpu().numpy())
    Xhat_scaled = np.concatenate(outs, axis=0)

    if scaler is not None and hasattr(scaler, "inverse_transform"):
        X_true = scaler.inverse_transform(X_scaled)
        X_hat  = scaler.inverse_transform(Xhat_scaled)
        err = np.mean((X_true - X_hat) ** 2, axis=1)
    else:
        X_true = X_hat = None
        err = np.mean((X_scaled - Xhat_scaled) ** 2, axis=1)

    return (err, X_true, X_hat) if return_recons else err




# 1️⃣ Entrena el AE y calcula thresholds con FRESH
re_fresh = recon_error_mse(trained_autoencoder, X_fresh)
Z_fresh = encode_latent(trained_autoencoder, X_fresh)
mu_f, Cinv_f = fit_gaussian(Z_fresh)
md_fresh = mahalanobis(Z_fresh, mu_f, Cinv_f)

TH_RE = np.percentile(re_fresh, 95)
TH_MAH = np.percentile(md_fresh, 95)

# 2️⃣ Evalúa otro dataset (aged/water)
re_aged = recon_error_mse(trained_autoencoder, X_aged)
Z_aged = encode_latent(trained_autoencoder, X_aged)
md_aged = mahalanobis(Z_aged, mu_f, Cinv_f)

flags_re = (re_aged > TH_RE).astype(int)
flags_mah = (md_aged > TH_MAH).astype(int)
flags_any = np.maximum(flags_re, flags_mah)

out_aged = sweep_aged_eni.copy()
out_aged["reconstruction_error"] = re_aged
out_aged["mahalanobis_latent"] = md_aged
out_aged["is_outlier_recon"] = flags_re
out_aged["is_outlier_latent"] = flags_mah
out_aged["is_outlier_any"] = flags_any





import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(7,5))

sns.scatterplot(
    data=out_fresh, 
    x="age_hours", 
    y="reconstruction_error", 
    hue="is_outlier_any", 
    style="is_outlier_any",
    palette={0: "steelblue", 1: "firebrick"},
    alpha=0.8,
    s=40
)

plt.title("Autoencoder Reconstruction Error vs Aging")
plt.xlabel("Age (hours)")
plt.ylabel("Reconstruction Error")
plt.legend(title="Outlier", labels=["Normal", "Outlier"])
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()










import numpy as np
import torch

@torch.no_grad()
def recon_error_mse(model, X_np: np.ndarray, batch_size: int = 1024):
    errs = []
    for i in range(0, len(X_np), batch_size):
        xb = torch.from_numpy(X_np[i:i+batch_size]).float()
        xh = model(xb)
        e = torch.mean((xb - xh)**2, dim=1).cpu().numpy()
        errs.append(e)
    return np.concatenate(errs, axis=0)

@torch.no_grad()
def encode_latent(model, X_np: np.ndarray, batch_size: int = 1024):
    Z = []
    for i in range(0, len(X_np), batch_size):
        xb = torch.from_numpy(X_np[i:i+batch_size]).float()
        zb = model.encoder(xb)
        Z.append(zb.cpu().numpy())
    return np.concatenate(Z, axis=0)

def fit_gaussian(Z: np.ndarray, eps: float = 1e-6):
    mu = Z.mean(axis=0)
    C  = np.cov(Z, rowvar=False) + eps * np.eye(Z.shape[1])
    Cinv = np.linalg.inv(C)
    return mu, Cinv

def mahalanobis(Z, mu, Cinv):
    D = Z - mu
    return np.sqrt(np.einsum("ij,jk,ik->i", D, Cinv, D))

# y_name elegido para que tu pipeline de features sea el correcto
y = "TAN"          # o "water_ppm"

# 2A) features + scaler con tu función
X_fresh, y_fresh, meta_fresh, scaler = self.prepare_subsets(
    df_raw=df_fresh, y_name=y, is_training=True
)

# 2B) scores en fresh
encoder_model.eval()
re_fresh = recon_error_mse(encoder_model, X_fresh)      # reconstruction error
Z_fresh  = encode_latent(encoder_model, X_fresh)        # latent
mu_f, Cinv_f = fit_gaussian(Z_fresh)                    # para Mahalanobis
md_fresh = mahalanobis(Z_fresh, mu_f, Cinv_f)

# 2C) umbrales (ajusta percentiles a tu tolerancia)
TH_RE  = np.percentile(re_fresh, 95)   # p95 o p99 si quieres más estricto
TH_MAH = np.percentile(md_fresh, 95)

import pandas as pd

flags_re  = (re_fresh > TH_RE).astype(int)
flags_mah = (md_fresh > TH_MAH).astype(int)
flags_any = np.maximum(flags_re, flags_mah)

out_fresh = meta_fresh.copy()  # contiene sweep_new, age_hours y targets que existan
out_fresh["reconstruction_error"] = re_fresh
out_fresh["mahalanobis_latent"]   = md_fresh
out_fresh["is_outlier_recon"]     = flags_re
out_fresh["is_outlier_latent"]    = flags_mah
out_fresh["is_outlier_any"]       = flags_any

# opcional: ver los peores
out_fresh.sort_values("reconstruction_error", ascending=False).head(10)
# guardar
# out_fresh.to_csv("fresh_outliers.csv", index=False)