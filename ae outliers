import umap.umap_ as umap
import matplotlib.pyplot as plt
import numpy as np

# Latent embeddings del conjunto fresh
Z_fresh = encode_latent(trained_autoencoder, X_fresh)

# Reducir con UMAP
reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='euclidean', random_state=42)
Z_umap = reducer.fit_transform(Z_fresh)

# Agregar coordenadas al dataframe
out_fresh["umap1"] = Z_umap[:,0]
out_fresh["umap2"] = Z_umap[:,1]

# Plot
plt.figure(figsize=(7,6))
plt.scatter(
    out_fresh["umap1"], out_fresh["umap2"],
    c="lightgray", s=25, label="Normal points", alpha=0.6
)
plt.scatter(
    out_fresh.loc[out_fresh["is_outlier_any"]==1, "umap1"],
    out_fresh.loc[out_fresh["is_outlier_any"]==1, "umap2"],
    c="red", s=40, label="Outliers", alpha=0.9
)
plt.title("UMAP — Outlier detection on fresh samples")
plt.xlabel("UMAP1")
plt.ylabel("UMAP2")
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()





import umap.umap_ as umap
import matplotlib.pyplot as plt

# Embeddings del AE
Z_fresh = encode_latent(trained_autoencoder, X_fresh)
Z_eni   = encode_latent(trained_autoencoder, X_eni)

# Reducimos con UMAP
reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='euclidean', random_state=42)
Z_all = np.vstack([Z_fresh, Z_eni])
embedding = reducer.fit_transform(Z_all)

# Separar embeddings proyectados
n_fresh = len(Z_fresh)
E_fresh = embedding[:n_fresh]
E_eni   = embedding[n_fresh:]

# Plot con color por water
plt.figure(figsize=(7,6))
plt.scatter(E_fresh[:,0], E_fresh[:,1], s=20, alpha=0.3, color='gray', label='fresh')

sc = plt.scatter(
    E_eni[:,0], E_eni[:,1],
    c=sweep_aged_eni["water_ppm"].values,
    cmap='plasma', s=20, alpha=0.9,
    label='eni (water)'
)

plt.colorbar(sc, label="Water (ppm)")
plt.legend()
plt.title("Latent space (UMAP) — fresh vs eni colored by water")
plt.xlabel("UMAP1")
plt.ylabel("UMAP2")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()







plt.figure(figsize=(6.5,6))

# los fresh quedan como fondo
plt.scatter(P_fresh[:,0], P_fresh[:,1], s=16, alpha=0.25, color='gray', label='fresh')

# ENI coloreado por age_hours
sc = plt.scatter(
    P_eni[:,0], P_eni[:,1],
    c=meta_eni["age_hours"].values,
    cmap="plasma",
    s=18, alpha=0.9, label="eni (aged/water)"
)

plt.colorbar(sc, label="Age (hours)")
plt.title("Latent space (PCA) — ENI colored by age_hours")
plt.xlabel("PC1"); plt.ylabel("PC2")
plt.legend(); plt.grid(alpha=0.3); plt.tight_layout()
plt.show()







from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

# 1) Prepara features con TU pipeline
y = config["y_name"]  # "TAN" o "water_ppm"

# calibras/fiteas scaler con FRESH
X_fresh, y_fresh, meta_fresh, scaler = self.prepare_subsets(
    df_raw=df_fresh, y=y, is_training=True, scaler=None
)

# usas EL MISMO scaler para ENI (aged/water)
X_eni, y_eni, meta_eni, _ = self.prepare_subsets(
    df_raw=df_eni, y=y, is_training=False, scaler=scaler
)

# 2) Embeddings latentes
Z_fresh = encode_latent(trained_autoencoder, X_fresh)
Z_eni   = encode_latent(trained_autoencoder, X_eni)

# 3) PCA en el conjunto combinado (para comparar en el mismo plano)
Z_all = np.vstack([Z_fresh, Z_eni])
P_all = PCA(n_components=2).fit_transform(Z_all)
n_f = len(Z_fresh)

P_fresh = P_all[:n_f]
P_eni   = P_all[n_f:]

# 4) Plot: fresh vs eni
plt.figure(figsize=(6.5,6))
plt.scatter(P_fresh[:,0], P_fresh[:,1], s=16, alpha=0.7, label="fresh")
plt.scatter(P_eni[:,0],   P_eni[:,1],   s=16, alpha=0.7, label="eni (aged/water)")
plt.title("Latent space (PCA) — fresh vs eni")
plt.xlabel("PC1"); plt.ylabel("PC2")
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()








def score_outliers_with_pipeline(df_any, y, model, scaler, TH_RE,
                                 mu_latent=None, Cinv_latent=None, TH_MAH=None):
    # usa tu función; OJO: is_training=False y pasa el mismo scaler
    X_any, y_any, meta_any, _ = self.prepare_subsets(
        df_raw=df_any, y=y, is_training=False, scaler=scaler
    )

    # reconstruction error (dominio original)
    re = recon_error_mse(model, X_any, scaler=scaler)

    # flags por reconstrucción
    is_out_re = (re > TH_RE).astype(int)

    # si quieres latente:
    if mu_latent is not None and Cinv_latent is not None and TH_MAH is not None:
        Z  = encode_latent(model, X_any)
        md = mahalanobis(Z, mu_latent, Cinv_latent)
        is_out_md = (md > TH_MAH).astype(int)
    else:
        md = np.zeros_like(re)
        is_out_md = np.zeros_like(is_out_re)

    # combine
    is_any = np.maximum(is_out_re, is_out_md)

    # salida: usa sweep_aged_y que tu prepare_subsets ya devuelve como meta
    out = meta_any.copy()
    out["reconstruction_error"] = re
    out["mahalanobis_latent"]   = md
    out["is_outlier_recon"]     = is_out_re
    out["is_outlier_latent"]    = is_out_md
    out["is_outlier_any"]       = is_any
    return out



@torch.no_grad()
def encode_latent(model, X_scaled: np.ndarray, batch_size: int = 1024):
    model.eval()
    Zs = []
    for i in range(0, len(X_scaled), batch_size):
        xb = torch.from_numpy(X_scaled[i:i+batch_size]).float()
        zb = model.encoder(xb)
        Zs.append(zb.cpu().numpy())
    return np.concatenate(Zs, axis=0)

def fit_gaussian(Z, eps=1e-6):
    mu = Z.mean(axis=0)
    C  = np.cov(Z, rowvar=False) + eps*np.eye(Z.shape[1])
    Cinv = np.linalg.inv(C)
    return mu, Cinv

def mahalanobis(Z, mu, Cinv):
    D = Z - mu
    return np.sqrt(np.einsum("ij,jk,ik->i", D, Cinv, D))

import numpy as np
import torch

@torch.no_grad()
def recon_error_mse(model, X_scaled: np.ndarray, scaler=None, batch_size: int = 1024,
                    return_recons: bool = False):
    model.eval()
    outs = []
    for i in range(0, len(X_scaled), batch_size):
        xb = torch.from_numpy(X_scaled[i:i+batch_size]).float()
        xh = model(xb)
        outs.append(xh.cpu().numpy())
    Xhat_scaled = np.concatenate(outs, axis=0)

    if scaler is not None and hasattr(scaler, "inverse_transform"):
        X_true = scaler.inverse_transform(X_scaled)
        X_hat  = scaler.inverse_transform(Xhat_scaled)
        err = np.mean((X_true - X_hat) ** 2, axis=1)
    else:
        X_true = X_hat = None
        err = np.mean((X_scaled - Xhat_scaled) ** 2, axis=1)

    return (err, X_true, X_hat) if return_recons else err




# 1️⃣ Entrena el AE y calcula thresholds con FRESH
re_fresh = recon_error_mse(trained_autoencoder, X_fresh)
Z_fresh = encode_latent(trained_autoencoder, X_fresh)
mu_f, Cinv_f = fit_gaussian(Z_fresh)
md_fresh = mahalanobis(Z_fresh, mu_f, Cinv_f)

TH_RE = np.percentile(re_fresh, 95)
TH_MAH = np.percentile(md_fresh, 95)

# 2️⃣ Evalúa otro dataset (aged/water)
re_aged = recon_error_mse(trained_autoencoder, X_aged)
Z_aged = encode_latent(trained_autoencoder, X_aged)
md_aged = mahalanobis(Z_aged, mu_f, Cinv_f)

flags_re = (re_aged > TH_RE).astype(int)
flags_mah = (md_aged > TH_MAH).astype(int)
flags_any = np.maximum(flags_re, flags_mah)

out_aged = sweep_aged_eni.copy()
out_aged["reconstruction_error"] = re_aged
out_aged["mahalanobis_latent"] = md_aged
out_aged["is_outlier_recon"] = flags_re
out_aged["is_outlier_latent"] = flags_mah
out_aged["is_outlier_any"] = flags_any





import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(7,5))

sns.scatterplot(
    data=out_fresh, 
    x="age_hours", 
    y="reconstruction_error", 
    hue="is_outlier_any", 
    style="is_outlier_any",
    palette={0: "steelblue", 1: "firebrick"},
    alpha=0.8,
    s=40
)

plt.title("Autoencoder Reconstruction Error vs Aging")
plt.xlabel("Age (hours)")
plt.ylabel("Reconstruction Error")
plt.legend(title="Outlier", labels=["Normal", "Outlier"])
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()










import numpy as np
import torch

@torch.no_grad()
def recon_error_mse(model, X_np: np.ndarray, batch_size: int = 1024):
    errs = []
    for i in range(0, len(X_np), batch_size):
        xb = torch.from_numpy(X_np[i:i+batch_size]).float()
        xh = model(xb)
        e = torch.mean((xb - xh)**2, dim=1).cpu().numpy()
        errs.append(e)
    return np.concatenate(errs, axis=0)

@torch.no_grad()
def encode_latent(model, X_np: np.ndarray, batch_size: int = 1024):
    Z = []
    for i in range(0, len(X_np), batch_size):
        xb = torch.from_numpy(X_np[i:i+batch_size]).float()
        zb = model.encoder(xb)
        Z.append(zb.cpu().numpy())
    return np.concatenate(Z, axis=0)

def fit_gaussian(Z: np.ndarray, eps: float = 1e-6):
    mu = Z.mean(axis=0)
    C  = np.cov(Z, rowvar=False) + eps * np.eye(Z.shape[1])
    Cinv = np.linalg.inv(C)
    return mu, Cinv

def mahalanobis(Z, mu, Cinv):
    D = Z - mu
    return np.sqrt(np.einsum("ij,jk,ik->i", D, Cinv, D))

# y_name elegido para que tu pipeline de features sea el correcto
y = "TAN"          # o "water_ppm"

# 2A) features + scaler con tu función
X_fresh, y_fresh, meta_fresh, scaler = self.prepare_subsets(
    df_raw=df_fresh, y_name=y, is_training=True
)

# 2B) scores en fresh
encoder_model.eval()
re_fresh = recon_error_mse(encoder_model, X_fresh)      # reconstruction error
Z_fresh  = encode_latent(encoder_model, X_fresh)        # latent
mu_f, Cinv_f = fit_gaussian(Z_fresh)                    # para Mahalanobis
md_fresh = mahalanobis(Z_fresh, mu_f, Cinv_f)

# 2C) umbrales (ajusta percentiles a tu tolerancia)
TH_RE  = np.percentile(re_fresh, 95)   # p95 o p99 si quieres más estricto
TH_MAH = np.percentile(md_fresh, 95)

import pandas as pd

flags_re  = (re_fresh > TH_RE).astype(int)
flags_mah = (md_fresh > TH_MAH).astype(int)
flags_any = np.maximum(flags_re, flags_mah)

out_fresh = meta_fresh.copy()  # contiene sweep_new, age_hours y targets que existan
out_fresh["reconstruction_error"] = re_fresh
out_fresh["mahalanobis_latent"]   = md_fresh
out_fresh["is_outlier_recon"]     = flags_re
out_fresh["is_outlier_latent"]    = flags_mah
out_fresh["is_outlier_any"]       = flags_any

# opcional: ver los peores
out_fresh.sort_values("reconstruction_error", ascending=False).head(10)
# guardar
# out_fresh.to_csv("fresh_outliers.csv", index=False)