# =======================
# 1. Select EIS feature columns
# =======================
# Assumes all columns EXCEPT metadata are EIS features
eis_cols = [c for c in df_data_fresh.columns 
            if ("real" in c) or ("imag" in c) or ("epr" in c)]

# =======================
# 2. Merge metadata with EIS matrices
# =======================

# Merge fresh
df_fresh_full = df_data_fresh.merge(
    sweep_aged[["sweep_new", "age_hours", "water_ppm"]],
    on="sweep_new",
    how="left"
)

# Merge aged/contaminated
df_eni_full = df_data_eni.merge(
    sweep_aged_eni[["sweep_new", "age_hours", "water_ppm"]],
    on="sweep_new",
    how="left"
)

# Combine both sources
df_all = pd.concat([df_fresh_full, df_eni_full], ignore_index=True)

# =======================
# 3. Encode EIS signals into latent space
# =======================
X_all = df_all[eis_cols].to_numpy()

Z = encode_latent(trained_autoencoder, X_all)
latent_dim = Z.shape[1]

latent_df = pd.DataFrame(Z, columns=[f"z{i+1}" for i in range(latent_dim)])

# =======================
# 4. Build a combined table (latent + physical metadata)
# =======================
combined = pd.concat([latent_df,
                      df_all[["age_hours", "water_ppm"]].reset_index(drop=True)],
                     axis=1)

# =======================
# 5. Compute correlations
# =======================
corr = combined.corr(method="spearman")

# =======================
# 6. Plot latent–feature correlation heatmap
# =======================
import seaborn as sns
plt.figure(figsize=(10,6))

sns.heatmap(
    corr.loc[["age_hours", "water_ppm"], latent_df.columns],
    annot=True, fmt=".2f",
    cmap="mako"
)

plt.title("Correlation between Latent Dimensions and Physical Features")
plt.xlabel("Latent dimensions")
plt.ylabel("Physical features")
plt.tight_layout()
plt.show()








import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# -------------------------------------------------------------
# 1. Combine fresh + aged/contaminated datasets (same AE features)
# -------------------------------------------------------------
df_all = pd.concat([df_data_fresh, df_data_eni], ignore_index=True)

# AE feature columns → everything that belongs to EIS measurements
feature_cols = [c for c in df_all.columns if ("real" in c.lower() or 
                                              "imag" in c.lower() or 
                                              "epr" in c.lower())]

# -------------------------------------------------------------
# 2. Encode all samples into latent space
#    The autoencoder takes only numeric EIS features
# -------------------------------------------------------------
X_np = df_all[feature_cols].to_numpy().astype(np.float32)

Z = encode_latent(trained_autoencoder, X_np)
latent_dim = Z.shape[1]

latent_df = pd.DataFrame(Z, columns=[f"z{i+1}" for i in range(latent_dim)])

# -------------------------------------------------------------
# 3. Attach physical metadata (if available)
#    Only include columns that exist in your dataset
# -------------------------------------------------------------
meta_cols = [c for c in ["water_ppm", "age_hours", "sweep_new"] if c in df_all.columns]

full_df = pd.concat([latent_df, df_all[meta_cols].reset_index(drop=True)], axis=1)

# -------------------------------------------------------------
# 4. Compute Spearman correlation (robust for nonlinear behavior)
# -------------------------------------------------------------
corr = full_df.corr(method="spearman")

plt.figure(figsize=(14, 8))
sns.heatmap(corr, cmap="mako", annot=True, fmt=".2f")
plt.title("Correlation of Latent Dimensions with Physical Features", fontsize=14)
plt.xlabel("Latent dimensions")
plt.ylabel("Physical features")
plt.tight_layout()
plt.show()










import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# ---------------------------------------------------
# 1. Select EIS feature columns used by the autoencoder
# ---------------------------------------------------
# Assumption: sweep_age and sweep_age_eni already exist
# and contain EIS features + metadata columns:
#   - water_ppm
#   - age_hours
#   - sweep_new
#   - is_outlier_any (optional)
#   - reconstruction_error (optional)
feature_cols = [
    c for c in sweep_age.columns
    if c not in ['water_ppm', 'age_hours', 'sweep_new',
                 'is_outlier_any', 'reconstruction_error']
]

# ---------------------------------------------------
# 2. Combine fresh and aged/contaminated datasets
# ---------------------------------------------------
df_all = pd.concat([sweep_age, sweep_age_eni], ignore_index=True)

# ---------------------------------------------------
# 3. Encode all samples into the latent space
# ---------------------------------------------------
# encode_latent(trained_autoencoder, X) should return
# a NumPy array of shape [n_samples, latent_dim]
Z = encode_latent(trained_autoencoder, df_all[feature_cols])

latent_dim = Z.shape[1]
latent_df = pd.DataFrame(
    Z,
    columns=[f"z{i+1}" for i in range(latent_dim)]
)

# ---------------------------------------------------
# 4. Build a single dataframe with latent + physical features
# ---------------------------------------------------
meta_cols = ['water_ppm', 'age_hours', 'sweep_new']
full_df = pd.concat(
    [
        latent_df.reset_index(drop=True),
        df_all[meta_cols].reset_index(drop=True)
    ],
    axis=1
)

# ---------------------------------------------------
# 5. Compute correlation matrix (Spearman is safer for monotonic trends)
# ---------------------------------------------------
corr = full_df.corr(method='spearman')

# ---------------------------------------------------
# 6. Extract correlations: latent dimensions vs physical features
# ---------------------------------------------------
rows = meta_cols
cols = [f"z{i+1}" for i in range(latent_dim)]

corr_subset = corr.loc[rows, cols]
print("Spearman correlation between physical features and latent dimensions:")
print(corr_subset)

# ---------------------------------------------------
# 7. Plot heatmap for easier interpretation
# ---------------------------------------------------
plt.figure(figsize=(10, 7))
sns.heatmap(
    corr_subset,
    annot=True,
    cmap='magma',
    linewidths=0.5,
    fmt='.2f'
)
plt.title("Latent dimensions vs physical features (Spearman correlation)")
plt.xlabel("Latent dimensions")
plt.ylabel("Physical features")
plt.tight_layout()
plt.show()







# z_fresh: matriz (n_samples, latent_dim)
# df_meta: dataframe con water_ppm, age_hours, etc.

latent_df = pd.DataFrame(z_fresh, columns=[f"z{i+1}" for i in range(z_fresh.shape[1])])

full_df = pd.concat([latent_df, df_meta.reset_index(drop=True)], axis=1)

corr = full_df.corr()

plt.figure(figsize=(10,7))
sns.heatmap(corr.loc[['water_ppm','age_hours'], latent_df.columns],
            annot=True, cmap="mako", fmt=".2f")
plt.title("Correlation of latent dimensions with physical features")
plt.show()










plt.figure(figsize=(8,6))

# Fresh
plt.scatter(out_fresh["water_ppm"],
            out_fresh["reconstruction_error"],
            c="dodgerblue", label="Fresh", s=70)

# Aged / contaminated
plt.scatter(out_eni["water_ppm"],
            out_eni["reconstruction_error"],
            c="crimson", marker="x", label="Aged / Contaminated", s=90)

plt.yscale("log")
plt.xlabel("Water content (ppm)")
plt.ylabel("Reconstruction error (MSE, log scale)")
plt.title("Fresh vs Aged/Contaminated — Reconstruction Error Comparison")
plt.grid(alpha=.3)
plt.legend()
plt.show()









import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Combine fresh + aged
df = pd.concat([
    out_fresh.assign(dataset="fresh"),
    out_eni.assign(dataset="aged")
], ignore_index=True)

# Create bins for water ppm
bins = [0, 1, 1000, 2000, 3000, 5000]
labels = ["fresh (0)", "0–1k", "1–2k", "2–3k", "3–5k"]

df["water_bin"] = pd.cut(df["water_ppm"], bins=bins, labels=labels, include_lowest=True)

# Compute mean reconstruction error per bin & per dataset
heat = df.groupby(["dataset", "water_bin"])["reconstruction_error"].mean().unstack()

plt.figure(figsize=(10,5))
sns.heatmap(heat, annot=True, fmt=".1f", cmap="magma", linewidths=.5)
plt.title("Mean Reconstruction Error — Fresh vs Aged by Water Content")
plt.ylabel("Dataset")
plt.xlabel("Water level (ppm)")
plt.show()











import pandas as pd
import matplotlib.pyplot as plt

# --- Group reconstruction error by water_ppm levels ---
df = out_eni.copy()   # aged/water-mixed samples
df = df[df["water_ppm"] > 0]   # aseguramos que no incluya fresh

grouped = df.groupby("water_ppm")["reconstruction_error"].mean()

print("Mean reconstruction error by water_ppm:\n")
print(grouped)

# --- Plot ---
plt.figure(figsize=(7,5))
plt.plot(grouped.index, grouped.values, marker="o", linewidth=2)
plt.xlabel("Water content (ppm)")
plt.ylabel("Mean reconstruction error (MSE)")
plt.title("Dose–Response: Reconstruction Error vs Water Content")
plt.grid(alpha=0.3)
plt.show()








plt.figure(figsize=(8,6))

# Colors based on outlier flag
colors_fresh = ["red" if x==1 else "blue" for x in out_fresh["is_outlier_any"]]
colors_eni   = ["red" if x==1 else "blue" for x in out_eni["is_outlier_any"]]

# Fresh samples
plt.scatter(
    out_fresh["water_ppm"],
    out_fresh["reconstruction_error"],
    c=colors_fresh,
    s=70,
    alpha=0.8,
    marker="o",
    label="Fresh (0 water / 0 age)"
)

# Aged / contaminated samples
plt.scatter(
    out_eni["water_ppm"],
    out_eni["reconstruction_error"],
    c=colors_eni,
    s=80,
    alpha=0.9,
    marker="x",
    label="Aged / Water-mixed"
)

plt.yscale("log")  # super importante para que se vea todo

plt.xlabel("Water content (ppm)")
plt.ylabel("Reconstruction error (MSE, log scale)")
plt.title("Fresh vs Aged/Contaminated — Reconstruction Error (Normal vs Outlier)")

# Custom legend for colors
import matplotlib.patches as mpatches
normal_patch = mpatches.Patch(color='blue', label='Normal')
outlier_patch = mpatches.Patch(color='red', label='Outlier')
plt.legend(handles=[normal_patch, outlier_patch], loc="upper left")

plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()









plt.figure(figsize=(8,6))

# Fresh samples (marker = circle)
plt.scatter(
    out_fresh["water_ppm"],
    out_fresh["reconstruction_error"],
    c=out_fresh["is_outlier_any"],
    cmap="coolwarm",
    s=70,
    alpha=0.8,
    marker="o",
    label="Fresh (0 water / 0 age)"
)

# Aged / ENI samples (marker = X)
plt.scatter(
    out_eni["water_ppm"],
    out_eni["reconstruction_error"],
    c=out_eni["is_outlier_any"],
    cmap="coolwarm",
    s=80,
    alpha=0.9,
    marker="x",
    label="Aged / Water-mixed samples"
)

plt.yscale("log")   # SUPER importante so they don't get flattened

plt.xlabel("Water content (ppm)")
plt.ylabel("Reconstruction error (MSE, log scale)")
plt.title("Fresh vs Contaminated — Reconstruction Error with Outlier Flag")

cbar = plt.colorbar()
cbar.set_label("Outlier flag (0=normal, 1=outlier)")

plt.grid(alpha=0.3)
plt.legend()
plt.tight_layout()
plt.show()











import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

# agrupamos por sweep y resumimos
summary = out_fresh.groupby("sweep_new").agg({
    "is_outlier_any": "mean",
    "reconstruction_error": "mean",
    "water_ppm": "mean"
}).reset_index()

# solo sweeps con outliers
summary_out = summary[summary["is_outlier_any"] > 0].sort_values("reconstruction_error", ascending=False)

plt.figure(figsize=(8, len(summary_out) * 0.3 + 2))
sns.heatmap(summary_out.set_index("sweep_new")[["reconstruction_error", "water_ppm"]],
            cmap="rocket_r", annot=True, fmt=".1f")
plt.title("Sweeps with outliers — mean reconstruction error & water_ppm")
plt.tight_layout()
plt.show()









import seaborn as sns
import pandas as pd

# Agrupamos por sweep
summary = out_fresh.groupby("sweep_new").agg({
    "is_outlier_any":"mean",
    "reconstruction_error":"mean",
    "water_ppm":"mean"
}).reset_index()

plt.figure(figsize=(8,5))
sns.heatmap(summary.pivot_table(index="sweep_new", values=["is_outlier_any","reconstruction_error","water_ppm"]),
            cmap="mako", annot=True, fmt=".2f")
plt.title("Sweep-level summary (mean values)")
plt.tight_layout()
plt.show()









import matplotlib.pyplot as plt

plt.figure(figsize=(7,5))
plt.scatter(out_fresh["water_ppm"], out_fresh["reconstruction_error"],
            c=out_fresh["is_outlier_any"], cmap="coolwarm", s=50, alpha=0.8)
plt.colorbar(label="Outlier flag (0=normal, 1=outlier)")
plt.xlabel("Water (ppm)")
plt.ylabel("Reconstruction error (MSE)")
plt.title("Reconstruction error vs water content (fresh samples)")
plt.grid(alpha=0.3)
plt.show()










import numpy as np

def group_band_residual(indexes, fmin, fmax):
    pos = [X_fresh_df.index.get_loc(i) for i in indexes]
    ridx = [X_fresh_df.columns.get_loc(c) for c in real_cols]
    iidx = [X_fresh_df.columns.get_loc(c) for c in imag_cols]
    f = np.array(freqs)
    band = (f>=fmin)&(f<=fmax)

    R = np.sqrt((X_true[pos][:,ridx][:,band] - X_hat[pos][:,ridx][:,band])**2 +
                (X_true[pos][:,iidx][:,band] - X_hat[pos][:,iidx][:,band])**2)
    return R.mean()  # residual promedio en la banda

idx_out = outs.index
idx_ok  = norm.index

print("Residual mean (outliers)  LF[0.1–10Hz]: ", group_band_residual(idx_out, 0.1, 10))
print("Residual mean (normals)   LF[0.1–10Hz]: ", group_band_residual(idx_ok,  0.1, 10))
print("Residual mean (outliers)  HF[1k–100k]:  ", group_band_residual(idx_out, 1e3, 1e5))
print("Residual mean (normals)   HF[1k–100k]:  ", group_band_residual(idx_ok,  1e3, 1e5))









outs = out_fresh[out_fresh["is_outlier_any"]==1]
norm = out_fresh[out_fresh["is_outlier_any"]==0]

print("water outliers vs normals:\n",
      outs["water_ppm"].describe(), "\n---\n", norm["water_ppm"].describe())




outliers = out_fresh[out_fresh["is_outlier_any"] == 1]
non_outliers = out_fresh[out_fresh["is_outlier_any"] == 0]

print(outliers[["sweep_new","water_ppm","age_hours"]].describe())




import matplotlib.pyplot as plt

# ordenar columnas (usa tu código de antes)
real_cols = sorted([c for c in X_fresh_df.columns if c.endswith("_real")], key=lambda c: float(c.split("_")[0]))
imag_cols = sorted([c for c in X_fresh_df.columns if c.endswith("_imag")], key=lambda c: float(c.split("_")[0]))
freqs = [float(c.split("_")[0]) for c in real_cols]

# indices
idx_outs = outliers.index[:3]
idx_norms = non_outliers.index[:3]

plt.figure(figsize=(8,6))
for idx in idx_norms:
    pos = X_fresh_df.index.get_loc(idx)
    r = X_true[pos, [X_fresh_df.columns.get_loc(c) for c in real_cols]]
    i = X_true[pos, [X_fresh_df.columns.get_loc(c) for c in imag_cols]]
    plt.plot(r, -i, 'gray', alpha=0.4)

for idx in idx_outs:
    pos = X_fresh_df.index.get_loc(idx)
    r = X_true[pos, [X_fresh_df.columns.get_loc(c) for c in real_cols]]
    i = X_true[pos, [X_fresh_df.columns.get_loc(c) for c in imag_cols]]
    plt.plot(r, -i, 'r', alpha=0.7)

plt.title("Outliers (rojo) vs normales (gris)")
plt.xlabel("Z' (real)"); plt.ylabel("-Z'' (imag)")
plt.grid(alpha=0.3); plt.show()








@torch.no_grad()
def recon_error_mse(model, X_np: np.ndarray, batch_size: int = 1024, return_recons=False, scaler=None):
    errs = []
    X_true, X_hat = [], []
    for i in range(0, len(X_np), batch_size):
        xb = torch.from_numpy(X_np[i:i+batch_size]).float()
        xh = model(xb)
        e = torch.mean((xb - xh)**2, dim=1).cpu().numpy()
        errs.append(e)
        if return_recons:
            X_true.append(xb.cpu().numpy())
            X_hat.append(xh.cpu().numpy())
    errs = np.concatenate(errs, axis=0)
    if not return_recons:
        return errs
    else:
        X_true = np.concatenate(X_true, axis=0)
        X_hat = np.concatenate(X_hat, axis=0)
        # si tus datos estaban escalados, podés desescalarlos aquí
        if scaler is not None:
            X_true = scaler.inverse_transform(X_true)
            X_hat  = scaler.inverse_transform(X_hat)
        return errs, X_true, X_hat





# --- 1) prepara sets con TUS funciones ---
# Ojo: usa el mismo scaler que usaste para entrenar/calibrar
X_fresh_scaled, y_fresh, sweep_aged_fresh, X_fresh_df, scaler = data_etl.prepare_subsets(
    df_raw=df_fresh, y=y_name, is_training=False, scaler=scaler
)

# calcula reconstrucción en dominio original
re_fresh, X_true, X_hat = recon_error_mse(
    trained_autoencoder, X_fresh_scaled, scaler=scaler, return_recons=True
)

# arma el df de resultados (out_fresh) si no lo tienes ya
out_fresh = sweep_aged_fresh.copy()
out_fresh["reconstruction_error"] = re_fresh
# ... agrega aquí tus flags si quieres

# --- 2) selecciona índices: 1 normal y 1 outlier ---
normal_idx  = out_fresh.sort_values("reconstruction_error").index[0]
outlier_idx = out_fresh.sort_values("reconstruction_error", ascending=False).index[0]

# --- 3) identifica columnas real/imag del wide ---
real_cols = [c for c in X_fresh_df.columns if c.startswith("real_")]
imag_cols = [c for c in X_fresh_df.columns if c.startswith("imag_")]

# aseguremos el mismo orden en real/imag por frecuencia
def sort_by_freq(cols):
    def get_f(c):
        # espera nombres tipo 'real_0.100' / 'imag_10.000'
        return float(c.split("_", 1)[1])
    return sorted(cols, key=get_f)

real_cols = sort_by_freq(real_cols)
imag_cols = sort_by_freq(imag_cols)
freqs = [float(c.split("_",1)[1]) for c in real_cols]

# --- 4) extrae curvas original y reconstruida para cada muestra ---
import numpy as np
def row_arrays(idx):
    # fila en X_true/X_hat está alineada con X_fresh_df.index
    pos = X_fresh_df.index.get_loc(idx)
    r_true = X_true[pos, [X_fresh_df.columns.get_loc(c) for c in real_cols]]
    i_true = X_true[pos, [X_fresh_df.columns.get_loc(c) for c in imag_cols]]
    r_hat  = X_hat[pos,  [X_fresh_df.columns.get_loc(c) for c in real_cols]]
    i_hat  = X_hat[pos,  [X_fresh_df.columns.get_loc(c) for c in imag_cols]]
    return r_true, i_true, r_hat, i_hat

rN, iN, rNh, iNh = row_arrays(normal_idx)
rO, iO, rOh, iOh = row_arrays(outlier_idx)

# --- 5) plots: Nyquist y residuales ---
import matplotlib.pyplot as plt

def nyquist(ax, real, imag, label, lw=2, ls='-'):
    ax.plot(real, -imag, ls=ls, lw=lw, marker='o', ms=3, label=label)

fig, axs = plt.subplots(1, 2, figsize=(11,5))

# normal
nyquist(axs[0], rN,  iN,  "Normal (orig)")
nyquist(axs[0], rNh, iNh, "Normal (recon)", ls='--')
axs[0].set_title("Nyquist — normal sample")
axs[0].set_xlabel("Z' (real)"); axs[0].set_ylabel("-Z'' (imag)")
axs[0].legend(); axs[0].grid(alpha=0.3)

# outlier
nyquist(axs[1], rO,  iO,  "Outlier (orig)")
nyquist(axs[1], rOh, iOh, "Outlier (recon)", ls='--')
axs[1].set_title("Nyquist — outlier sample")
axs[1].set_xlabel("Z' (real)"); axs[1].set_ylabel("-Z'' (imag)")
axs[1].legend(); axs[1].grid(alpha=0.3)

plt.tight_layout(); plt.show()

# residuales por frecuencia (opcional):
fig, ax = plt.subplots(figsize=(7,4))
ax.plot(freqs, np.sqrt((rO-rOh)**2 + (iO-iOh)**2), marker='o')
ax.set_xscale('log')
ax.set_title("Outlier residual vs frequency")
ax.set_xlabel("Frequency (Hz)"); ax.set_ylabel("Residual magnitude")
ax.grid(alpha=0.3); plt.tight_layout(); plt.show()









import matplotlib.pyplot as plt

# --- selecciona índices ---
normal_idx = out_fresh[out_fresh["is_outlier_any"]==0].sample(1, random_state=42).index[0]
outlier_idx = out_fresh[out_fresh["is_outlier_any"]==1].sample(1, random_state=42).index[0]

# --- selecciona columnas reales e imaginarias ---
real_cols = [c for c in out_fresh.columns if c.startswith("real_")]
imag_cols = [c for c in out_fresh.columns if c.startswith("imag_")]

# --- valores ---
real_normal = out_fresh.loc[normal_idx, real_cols].values
imag_normal = out_fresh.loc[normal_idx, imag_cols].values
real_outlier = out_fresh.loc[outlier_idx, real_cols].values
imag_outlier = out_fresh.loc[outlier_idx, imag_cols].values

freqs = [float(c.split("_")[-1]) for c in real_cols]  # sacar frecuencia del nombre

# --- gráfico comparativo ---
plt.figure(figsize=(6,6))
plt.plot(real_normal, -imag_normal, 'o-', label="Normal sample", alpha=0.7)
plt.plot(real_outlier, -imag_outlier, 'o-', label="Outlier sample", alpha=0.8, color='red')
plt.xlabel("Z' (real)")
plt.ylabel("-Z'' (imag)")
plt.title("Nyquist plot comparison — normal vs outlier (fresh)")
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()




import umap.umap_ as umap
import matplotlib.pyplot as plt
import numpy as np

# Latent embeddings del conjunto fresh
Z_fresh = encode_latent(trained_autoencoder, X_fresh)

# Reducir con UMAP
reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='euclidean', random_state=42)
Z_umap = reducer.fit_transform(Z_fresh)

# Agregar coordenadas al dataframe
out_fresh["umap1"] = Z_umap[:,0]
out_fresh["umap2"] = Z_umap[:,1]

# Plot
plt.figure(figsize=(7,6))
plt.scatter(
    out_fresh["umap1"], out_fresh["umap2"],
    c="lightgray", s=25, label="Normal points", alpha=0.6
)
plt.scatter(
    out_fresh.loc[out_fresh["is_outlier_any"]==1, "umap1"],
    out_fresh.loc[out_fresh["is_outlier_any"]==1, "umap2"],
    c="red", s=40, label="Outliers", alpha=0.9
)
plt.title("UMAP — Outlier detection on fresh samples")
plt.xlabel("UMAP1")
plt.ylabel("UMAP2")
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()





import umap.umap_ as umap
import matplotlib.pyplot as plt

# Embeddings del AE
Z_fresh = encode_latent(trained_autoencoder, X_fresh)
Z_eni   = encode_latent(trained_autoencoder, X_eni)

# Reducimos con UMAP
reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric='euclidean', random_state=42)
Z_all = np.vstack([Z_fresh, Z_eni])
embedding = reducer.fit_transform(Z_all)

# Separar embeddings proyectados
n_fresh = len(Z_fresh)
E_fresh = embedding[:n_fresh]
E_eni   = embedding[n_fresh:]

# Plot con color por water
plt.figure(figsize=(7,6))
plt.scatter(E_fresh[:,0], E_fresh[:,1], s=20, alpha=0.3, color='gray', label='fresh')

sc = plt.scatter(
    E_eni[:,0], E_eni[:,1],
    c=sweep_aged_eni["water_ppm"].values,
    cmap='plasma', s=20, alpha=0.9,
    label='eni (water)'
)

plt.colorbar(sc, label="Water (ppm)")
plt.legend()
plt.title("Latent space (UMAP) — fresh vs eni colored by water")
plt.xlabel("UMAP1")
plt.ylabel("UMAP2")
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()







plt.figure(figsize=(6.5,6))

# los fresh quedan como fondo
plt.scatter(P_fresh[:,0], P_fresh[:,1], s=16, alpha=0.25, color='gray', label='fresh')

# ENI coloreado por age_hours
sc = plt.scatter(
    P_eni[:,0], P_eni[:,1],
    c=meta_eni["age_hours"].values,
    cmap="plasma",
    s=18, alpha=0.9, label="eni (aged/water)"
)

plt.colorbar(sc, label="Age (hours)")
plt.title("Latent space (PCA) — ENI colored by age_hours")
plt.xlabel("PC1"); plt.ylabel("PC2")
plt.legend(); plt.grid(alpha=0.3); plt.tight_layout()
plt.show()







from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

# 1) Prepara features con TU pipeline
y = config["y_name"]  # "TAN" o "water_ppm"

# calibras/fiteas scaler con FRESH
X_fresh, y_fresh, meta_fresh, scaler = self.prepare_subsets(
    df_raw=df_fresh, y=y, is_training=True, scaler=None
)

# usas EL MISMO scaler para ENI (aged/water)
X_eni, y_eni, meta_eni, _ = self.prepare_subsets(
    df_raw=df_eni, y=y, is_training=False, scaler=scaler
)

# 2) Embeddings latentes
Z_fresh = encode_latent(trained_autoencoder, X_fresh)
Z_eni   = encode_latent(trained_autoencoder, X_eni)

# 3) PCA en el conjunto combinado (para comparar en el mismo plano)
Z_all = np.vstack([Z_fresh, Z_eni])
P_all = PCA(n_components=2).fit_transform(Z_all)
n_f = len(Z_fresh)

P_fresh = P_all[:n_f]
P_eni   = P_all[n_f:]

# 4) Plot: fresh vs eni
plt.figure(figsize=(6.5,6))
plt.scatter(P_fresh[:,0], P_fresh[:,1], s=16, alpha=0.7, label="fresh")
plt.scatter(P_eni[:,0],   P_eni[:,1],   s=16, alpha=0.7, label="eni (aged/water)")
plt.title("Latent space (PCA) — fresh vs eni")
plt.xlabel("PC1"); plt.ylabel("PC2")
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()








def score_outliers_with_pipeline(df_any, y, model, scaler, TH_RE,
                                 mu_latent=None, Cinv_latent=None, TH_MAH=None):
    # usa tu función; OJO: is_training=False y pasa el mismo scaler
    X_any, y_any, meta_any, _ = self.prepare_subsets(
        df_raw=df_any, y=y, is_training=False, scaler=scaler
    )

    # reconstruction error (dominio original)
    re = recon_error_mse(model, X_any, scaler=scaler)

    # flags por reconstrucción
    is_out_re = (re > TH_RE).astype(int)

    # si quieres latente:
    if mu_latent is not None and Cinv_latent is not None and TH_MAH is not None:
        Z  = encode_latent(model, X_any)
        md = mahalanobis(Z, mu_latent, Cinv_latent)
        is_out_md = (md > TH_MAH).astype(int)
    else:
        md = np.zeros_like(re)
        is_out_md = np.zeros_like(is_out_re)

    # combine
    is_any = np.maximum(is_out_re, is_out_md)

    # salida: usa sweep_aged_y que tu prepare_subsets ya devuelve como meta
    out = meta_any.copy()
    out["reconstruction_error"] = re
    out["mahalanobis_latent"]   = md
    out["is_outlier_recon"]     = is_out_re
    out["is_outlier_latent"]    = is_out_md
    out["is_outlier_any"]       = is_any
    return out



@torch.no_grad()
def encode_latent(model, X_scaled: np.ndarray, batch_size: int = 1024):
    model.eval()
    Zs = []
    for i in range(0, len(X_scaled), batch_size):
        xb = torch.from_numpy(X_scaled[i:i+batch_size]).float()
        zb = model.encoder(xb)
        Zs.append(zb.cpu().numpy())
    return np.concatenate(Zs, axis=0)

def fit_gaussian(Z, eps=1e-6):
    mu = Z.mean(axis=0)
    C  = np.cov(Z, rowvar=False) + eps*np.eye(Z.shape[1])
    Cinv = np.linalg.inv(C)
    return mu, Cinv

def mahalanobis(Z, mu, Cinv):
    D = Z - mu
    return np.sqrt(np.einsum("ij,jk,ik->i", D, Cinv, D))

import numpy as np
import torch

@torch.no_grad()
def recon_error_mse(model, X_scaled: np.ndarray, scaler=None, batch_size: int = 1024,
                    return_recons: bool = False):
    model.eval()
    outs = []
    for i in range(0, len(X_scaled), batch_size):
        xb = torch.from_numpy(X_scaled[i:i+batch_size]).float()
        xh = model(xb)
        outs.append(xh.cpu().numpy())
    Xhat_scaled = np.concatenate(outs, axis=0)

    if scaler is not None and hasattr(scaler, "inverse_transform"):
        X_true = scaler.inverse_transform(X_scaled)
        X_hat  = scaler.inverse_transform(Xhat_scaled)
        err = np.mean((X_true - X_hat) ** 2, axis=1)
    else:
        X_true = X_hat = None
        err = np.mean((X_scaled - Xhat_scaled) ** 2, axis=1)

    return (err, X_true, X_hat) if return_recons else err




# 1️⃣ Entrena el AE y calcula thresholds con FRESH
re_fresh = recon_error_mse(trained_autoencoder, X_fresh)
Z_fresh = encode_latent(trained_autoencoder, X_fresh)
mu_f, Cinv_f = fit_gaussian(Z_fresh)
md_fresh = mahalanobis(Z_fresh, mu_f, Cinv_f)

TH_RE = np.percentile(re_fresh, 95)
TH_MAH = np.percentile(md_fresh, 95)

# 2️⃣ Evalúa otro dataset (aged/water)
re_aged = recon_error_mse(trained_autoencoder, X_aged)
Z_aged = encode_latent(trained_autoencoder, X_aged)
md_aged = mahalanobis(Z_aged, mu_f, Cinv_f)

flags_re = (re_aged > TH_RE).astype(int)
flags_mah = (md_aged > TH_MAH).astype(int)
flags_any = np.maximum(flags_re, flags_mah)

out_aged = sweep_aged_eni.copy()
out_aged["reconstruction_error"] = re_aged
out_aged["mahalanobis_latent"] = md_aged
out_aged["is_outlier_recon"] = flags_re
out_aged["is_outlier_latent"] = flags_mah
out_aged["is_outlier_any"] = flags_any





import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(7,5))

sns.scatterplot(
    data=out_fresh, 
    x="age_hours", 
    y="reconstruction_error", 
    hue="is_outlier_any", 
    style="is_outlier_any",
    palette={0: "steelblue", 1: "firebrick"},
    alpha=0.8,
    s=40
)

plt.title("Autoencoder Reconstruction Error vs Aging")
plt.xlabel("Age (hours)")
plt.ylabel("Reconstruction Error")
plt.legend(title="Outlier", labels=["Normal", "Outlier"])
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()










import numpy as np
import torch

@torch.no_grad()
def recon_error_mse(model, X_np: np.ndarray, batch_size: int = 1024):
    errs = []
    for i in range(0, len(X_np), batch_size):
        xb = torch.from_numpy(X_np[i:i+batch_size]).float()
        xh = model(xb)
        e = torch.mean((xb - xh)**2, dim=1).cpu().numpy()
        errs.append(e)
    return np.concatenate(errs, axis=0)

@torch.no_grad()
def encode_latent(model, X_np: np.ndarray, batch_size: int = 1024):
    Z = []
    for i in range(0, len(X_np), batch_size):
        xb = torch.from_numpy(X_np[i:i+batch_size]).float()
        zb = model.encoder(xb)
        Z.append(zb.cpu().numpy())
    return np.concatenate(Z, axis=0)

def fit_gaussian(Z: np.ndarray, eps: float = 1e-6):
    mu = Z.mean(axis=0)
    C  = np.cov(Z, rowvar=False) + eps * np.eye(Z.shape[1])
    Cinv = np.linalg.inv(C)
    return mu, Cinv

def mahalanobis(Z, mu, Cinv):
    D = Z - mu
    return np.sqrt(np.einsum("ij,jk,ik->i", D, Cinv, D))

# y_name elegido para que tu pipeline de features sea el correcto
y = "TAN"          # o "water_ppm"

# 2A) features + scaler con tu función
X_fresh, y_fresh, meta_fresh, scaler = self.prepare_subsets(
    df_raw=df_fresh, y_name=y, is_training=True
)

# 2B) scores en fresh
encoder_model.eval()
re_fresh = recon_error_mse(encoder_model, X_fresh)      # reconstruction error
Z_fresh  = encode_latent(encoder_model, X_fresh)        # latent
mu_f, Cinv_f = fit_gaussian(Z_fresh)                    # para Mahalanobis
md_fresh = mahalanobis(Z_fresh, mu_f, Cinv_f)

# 2C) umbrales (ajusta percentiles a tu tolerancia)
TH_RE  = np.percentile(re_fresh, 95)   # p95 o p99 si quieres más estricto
TH_MAH = np.percentile(md_fresh, 95)

import pandas as pd

flags_re  = (re_fresh > TH_RE).astype(int)
flags_mah = (md_fresh > TH_MAH).astype(int)
flags_any = np.maximum(flags_re, flags_mah)

out_fresh = meta_fresh.copy()  # contiene sweep_new, age_hours y targets que existan
out_fresh["reconstruction_error"] = re_fresh
out_fresh["mahalanobis_latent"]   = md_fresh
out_fresh["is_outlier_recon"]     = flags_re
out_fresh["is_outlier_latent"]    = flags_mah
out_fresh["is_outlier_any"]       = flags_any

# opcional: ver los peores
out_fresh.sort_values("reconstruction_error", ascending=False).head(10)
# guardar
# out_fresh.to_csv("fresh_outliers.csv", index=False)