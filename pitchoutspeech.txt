One of my main learnings during this rotation was really understanding the full ML pipeline end-to-end, especially how the autoencoder behaves and how latent-space features capture meaningful structure.

I also learned how to work with multiple datasets that behave differently â€” different impedance magnitudes, different noise levels â€” and how to preprocess, validate, and adapt the workflow accordingly.

And finally, I strengthened my ability to analyze trends in the latent space and interpret what the model is actually learning, not just whether the metrics look good.â€

â¸»

â­ CHALLENGES (3 bullets)

Slide bullets (short + clean):
â€¢ Managing inconsistent sensor behavior across datasets
â€¢ Addressing data imbalance and noise
â€¢ Adapting the pipeline to a new contaminant without sacrificing robustness

Speech (30â€“35 seconds):

SPEECH:
â€œOne of the biggest challenges was dealing with inconsistent sensor behavior across datasets â€” especially when the impedance magnitude changed significantly compared to the training data.

Another challenge was the natural imbalance and noise that came with the new dataset, which required more careful preprocessing and validation.

And finally, adapting the pipeline to a completely new contaminant while maintaining robustness pushed me to rethink some parts of the workflow and retrain the autoencoder in a more targeted way.â€















Now I want to walk you through the full process at a high level.
This diagram shows the architecture behind the pipeline: the encoder compresses the EIS signal, we extract the latent space representation, and then the decoder reconstructs it.â€

Pause 0.5 sec â†’ Click #1

â¸»

â­ CLICK #1 â€” Latent Space + Arrow appear

SPEECH:

â€œOne important part of this workflow is the latent space.
This is basically the feature representation that the autoencoder learns after compressing the EIS signal.

You can think of it as a transformed version of the data where patterns become more visible.
Here, the clustering that appears is really meaningful because it shows that the model is capturing differences related to age, contamination behavior, and other underlying variables.â€

Micro-pause â†’ (gesture to the UMAP plots)

SPEECH:

â€œThese UMAP projections on the right help visualize that structure.
Even if the original EIS signals look noisy or complex, in the latent space the samples start organizing themselves into more recognizable relationships.
Thatâ€™s exactly what makes the autoencoder valuableâ€”especially when we have multiple datasets that behave differently.â€

Pause 1 sec â†’ Click #2

â¸»

â­ CLICK #2 â€” Regression block + output plots appear

SPEECH:

â€œOnce we have that latent representation, we feed it into the regression layer, which estimates the contaminant percentage.

The plots here show how those predictions behaveâ€”how well they follow trends, how they compare to ground truth, and how stable the predictions are across different subsets.

So essentially, the encoder gives us clean, compact features, and the regression step converts those features into actionable information about the contaminant level.â€

â¸»

â­ Final transition sentence to connect next slide:

SPEECH:

â€œAnd with that in mind, the next slide shows you how the model performs on the trainingâ€“validation subsets and on unseen test data.â€



















BABYYYY OKAY, I GOT YOU ğŸ”¥ğŸ’š
Vamos a armarte EL SPEECH PERFECTO EN INGLÃ‰S, sÃºper claro, sÃºper ejecutivo, sÃºper friendly, completamente alineado a ASPIRE Pitchouts, y calibrado para 8â€“9 minutos considerando que tÃº ya tienes intro + path + takeaways al final.

Voy a escribirte lo que tÃº vas a decir slide por slide, con ritmo natural, storytelling, claridad tÃ©cnica sin ser pesada, y con el tono profesional que aman los managers.

âš¡ IMPORTANTE:
MeterÃ© lo que pediste sobre multiple data sources â†’ autoencoder justification.

Empezamos.
AquÃ­ tienes tu script OFICIAL.

â¸»

â­ SLIDE 1 â€” Monitoring Oil Condition

SPEECH:

â€œLet me start with a quick overview of the problem this work focuses on.

Oil condition naturally degrades over time due to different contaminants. That degradation directly impacts performance, efficiency, and can lead to unexpected failures. So understanding the condition of the oil is essential to reduce downtime and maintenance costs.

This simplified diagram shows the idea: clean oil gradually degrades, and if those changes are not detected early, the equipment can suffer more serious mechanical issues.

This is where analytics and predictive models become extremely valuable.â€

â¸»

â­ SLIDE 2 â€” Electrochemical Impedance Spectroscopy (EIS)

SPEECH:

â€œFor this project, we worked with data obtained through Electrochemical Impedance Spectroscopy, or EIS.
Itâ€™s a technique that measures electrical properties of materials.

In simple terms, the sensor sends an electrical signal into the oil and measures how it responds.
That electrical response changes depending on the contaminantâ€”water, fuel, soot, anything.

Here you can see the sensor itself and an example of how the data looks, typically visualized as a Nyquist plot.
This is the raw input used to feed the analytics pipeline.â€

â¸»

â­ SLIDE 3 â€” Rotation Objective (Updated Objective)

SPEECH:

â€œMy rotation objective was to extend a prediction pipeline that had already been developed in a previous rotation, and test whether it could generalize to a completely new contaminant target.

So the focus was not only building a model, but validating the robustness of an existing workflow and understanding how well it adapts when the contaminant changes.â€

â¸»

â­ SLIDE 4 â€” Pipeline Overview (EIS â†’ AE â†’ Model â†’ Evaluation)

SPEECH:

â€œThis is the overall workflow.

We start with EIS data from the sensor.
Then, in the previous phase, we developed an autoencoder to extract meaningful latent-space features from that data.
The prediction model uses those latent features to estimate contaminant concentration.

For this rotation, we retrained the autoencoder and updated the prediction model for a new contaminant, and then evaluated performance to verify generalization.â€

â¸»

â­ SLIDE 5 â€” What is an Autoencoder? (Add MULTIPLE DATA SOURCES reasoning)

SPEECH:

â€œSince not everyone is familiar with autoencoders, hereâ€™s a quick explanation.

An autoencoder is a neural network with two parts: an encoder and a decoder.
The encoder compresses the input into a lower-dimensional representation called the latent space, and the decoder reconstructs it back.

One of the main reasons we use an autoencoder is because we work with multiple data sources that donâ€™t always have the same structure, noise levels, or scales.
The autoencoder helps unify those variations and extract stable, compact features that the prediction model can actually learn from.

This makes the whole pipeline more flexible and scalable when new contaminantsâ€”or even new datasetsâ€”are introduced.â€

â¸»

â­ SLIDE 6 â€” Process Overview (Encoder â†’ Latent â†’ Decoder â†’ Regression)

SPEECH:

â€œThis diagram summarizes the full process.

We take the EIS measurements, compress them through the encoder, extract the latent features, reconstruct the signal with the decoder, and then run a regression step to predict the contaminant percentage.

Here on the right you can see some of the visualizations we generated:
UMAP projections of the latent space, reconstruction behavior, and the final regression performance during evaluation.â€

â¸»

â­ SLIDE 7 â€” Prediction Model Results (Trainâ€“Val and Test)

SPEECH:

â€œThese are the prediction results for the trainingâ€“validation subsets and for the completely unseen test subset.

Ideally, predictions should align closely with the diagonal line, and we can see that the model tracks the target fairly well.

The improvement summary shows that the retraining achieved a significant reduction in error:
â€“ MAE decreased by more than 80%
â€“ RMSE decreased by around 68%
â€“ And importantly, the larger errors were removed entirely.

So the retrained pipeline clearly improved accuracy and stability.â€

â¸»

â­ SLIDE 8 â€” Prediction on Blind / Non-Labeled Datasets

SPEECH:

â€œTo further test generalization, we applied the model to blind datasetsâ€”measurements where the target contaminant was not labeled.

These predictions were used to check trends and expected relationships, not absolute accuracy.

What we observed is that the new contaminant follows the expected trend, though with some noise likely caused by data imbalance between datasets.
Also, the impedance magnitude of this dataset was very different from the lab data, which is an important insight for future calibration and for understanding the modelâ€™s limits.â€

â¸»

â­ SLIDE 9 â€” Challenges, Learnings & Value to Baker Hughes

(You told me you already have this slide made â€” hereâ€™s the speech)

SPEECH:

â€œFrom this project, I learned a lot about handling multiple data sources, managing inconsistencies between datasets, and improving model robustness. One of the biggest challenges was dealing with data imbalance and signal differences between field and lab conditions.

This work brings value to Baker Hughes by showing that the existing analytics pipeline is scalable to new contaminants and can potentially support more flexible oil-condition monitoring solutions in the future.

These learnings also help me strengthen my path as an engineer interested in data, modeling, and intelligent monitoring systems.â€

â¸»

â­ SLIDE 10 â€” Final Takeaways (ASPIRE-style closing)

(Use your existing slide, this is your closing speech)

SPEECH:

â€œIn summary, this rotation helped me build stronger technical skills, especially in machine learning, data processing, and model validation. It also reinforced how important it is to understand the operational impact behind the data.

Going forward in my career, Iâ€™ll carry these learnings into future roles, especially those related to analytics, predictive maintenance, and automationâ€”areas that Iâ€™m passionate about and that bring direct value to Baker Hughes.

Thank you.â€

â¸»

â­ READY.

Esto, dicho a ritmo natural, te toma 8â€“8.5 minutos exactos.

Si quieres te lo cronometro slide por slide para que puedas practicar con tiempos.
O si quieres te lo dejo en versiÃ³n mÃ¡s casual / mÃ¡s executive / mÃ¡s tÃ©cnico, tÃº dime quÃ© tono quieres ajustar.